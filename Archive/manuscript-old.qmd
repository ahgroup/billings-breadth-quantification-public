---
title: |
  A novel approach for robust evaluation of broadly reactive
  influenza vaccine candidates
author: " "
format:
  docx: 
    toc: false
    number-sections: false
    reference-doc: "ms_template.docx"
bibliography:
  - "refs.bib"
#  - "pkgs.bib"
csl: "plos-computational-biology.csl"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	error = FALSE,
	knitr.graphics.error = FALSE
)
library(yaml, include.only = NULL)
library(knitr, include.only = NULL)
library(flextable)
library(softbib)
```

**Authors**  

<!-- TODO fix affilitations -->
* W. Zane Billings$^{1,2,\land}$ (ORCID: 0000-0002-0184-6134);
* Amanda L. Skarlupka$^{3, \dagger}$ (0000-0002-3654-9076);
* Savannah M. Hammerton$^{1, 2}$ (0000-0003-2231-3510);
* Hayley Hemme$^{1, 2}$ (0009-0002-6609-1390);
* Murphy John $^{1, 2}$ (0009-0009-9591-1117);
* Ted M. Ross$^{3, 4}$ (0000-0003-1947-7469);
* Andreas Handel$^{1, 2, \land}$ (0000-0002-4622-1146)

Maybe: Ben Cowling, Bingyi Yang, Lambodhar Damodaran, Sarah's group?

**Author affiliations**  

1. College of Public Health, University of Georgia, Athens, GA, USA.
2. Center for the Ecology of Infectious Diseases, The University of Georgia, Athens, GA, USA.
3. Center for Vaccines and Immunology, University of Georgia, Athens, GA, USA.
4. Florida Research \& Innovation Center, Cleveland Clinic, Port St. Lucie, FL, USA.

$\land$ Corresponding author: WZB (wesley.billings@uga.edu). Alternate corresponding author: AH (ahandel@uga.edu)

$\dagger$ Disclaimer: Amanda L. Skarlupka is currently an employee of the National Cancer Institute, National Institutes of Health, Bethesda, MD. This article was prepared while Amanda L. Skarlupka was employed at the University of Georgia. The opinions expressed in this article are the
author's own and do not reflect the view of the National Cancer Institute, the National Institutes of Health, the Department of Health and Human Services, or the United States government.

<!--
**Target journal:** PNAS or Proc Soc B? Wide readership so we need to reduce the technical complexity of this manuscript.
-->

\pagebreak

# Abstract

**Background:** Annual vaccines are the primary intervention for limiting the burden of influenza disease, but vaccine effectiveness (VE) depends on many factors, including how similar the strains used for vaccine formulation are to circulating influenza strains. Ensuring that the vaccine induces a broadly-reactive immune response to many possible strains of influenza is paramount for eliminating the deleterious effect of strain dissimilarity on VE. Measuring the breadth of response of a vaccine or candidate entails estimating the immunogenicity of the vaccine against many different influenza strains, and then combining these measurements. The most common method is the proportion of strains to which a vaccine induces seroconversion, which is strongly dependent on the panel of strains used for analysis. Vaccine breadth generated with this method cannot fairly be compared across different labs or even differen studies from the same lab using different virus panels. In our study, we develop a novel robust metric for quantifying vaccine breadth across labs and virus panels.

**Methods:**  We used data from a prospective, open annual influenza vaccination cohort across three study sites from Fall 2013/14 through Spring 2017/18. Each individual contributed pre-vaccination and post-vaccination serum samples which were used for a panel of hemagglutination inhibtion (HAI) assays against many strains of influenza, including the strains used in each season's vaccine. We computed multiple antigenic distance metrics between each HAI assay strain and the corresponding seasonal vaccine strain

**Results:** First, we calculated multiple antigenic distance metrics and fit summary antibody landscapes using a simple model. From these summary landscapes, we computed our novel metrics which adjust for antigenic distance and censoring in the reported titers. We also computed standard breadth metrics from the existing literature, along with adjusted metrics which are identical to the standard metrics but also control for censoring in the titers. We simulated data to represent a study across multiple labs with different HAI panels, and calculated the intraclass correlation (ICC) to measure how much each metric varied across labs. Finally, we subsampled multiple panels of heterologous strains and sets of individuals from our cohort study to create hypothetical studies, and we computed metrics across these subsampled studies.

**Conclusions:**

**Keywords:**

\pagebreak

# Introduction

A major challenge in developing a universal influenza vaccine is accounting for the rapid antigenic evolution of influenza virus [@kim2018; @erbelding2020; @wei2020; @vogel2020]. Many broadly-reactive vaccine candidates are under development, but the best method for assessing the breadth, i.e. how broadly-reactive a vaccine candidate is, of vaccine response is still unclear. Multiple assays have been proposed for measuring vaccine response [@krammer2019a], but hemagglutination inhibition (HAI) titer is currently the most common biomarker associated with protection from a given influenza strain, since an HAI titer of 1:40 was previously estimated as the 50% protective titer [@hobson1972; @coudeville2010]. HAI can be used to analyze the breadth of vaccine response by measuring the HAI titer against a panel of historical influenza viruses.

<!-- put this more in context of our breadth metric, we should maybe think
about establishing these in the context of breadth
Need to be more detailed and talk about this in terms of seroconversion
specifically.
-->
In previous research, the breadth of the response is quantified by measuring vaccine-induced immunogenicity against a panel of historical influenza strains. Among multiple vaccine candidates, the one that induces the highest proportion of positive immune responses is considered the most broadly reactive <!-- think about how to word this --> [@allen2021; @dugan2020; @hinojosa2020; @li2021; @jang2021; @boyoglu-barnum2021]. This method is easy to conduct, but the selection of different virus panels across research groups makes the results from different labs hard to compare. Some methodological work has focused on methods for imputing data across different virus panels (CITE TAL EINAV), but there is no practical comparison of how well these imputation measures work.

Recent methods have focused on quantitative analysis of individual antibody
landscapes, rather than counting the number of seroprotection events across an
HAI panel. An antibody landscape is a curve which depicts antibody response on
the $y$-axis with respect to some measurement on the $x$-axis which attempts to
order the responses by the differences between influenza strains (distance
metrics). Distance metrics of interest include the year of strain isolation
[@yang2020; @auladell2022], genetic differences [@nachbagauer2017; @smith2004a;
@gupta2006; @anderson2018; @forghani2020],
and distances derived from antigenic cartography [@fonville2014; @hinojosa2020;
@fonville2015; @kucharski2018; @hay2019; @wang2021].

In our study, we propose the use of antibody landscapes from influenza vaccine
cohort studies as the basis for measuring vaccine breadth. We implement methods
for creating population-summary antibody landscapes, and metrics derived from
the summary antibody landscape which are more robust to differences in virus
panels than previously used methods. We also include a case study showing how
our methods can be used to assess breadth of response between two different
vaccine candidates. Overall, our study contributes to the definition of breadth
and the reproducibility of selecting broadly-reactive vaccine candidates by
improving the ability to compare results across labs.


# Methods

## Data source

We performed a secondary data analysis of the UGAFluVac cohort data. The cohort
is a prospective, open, non-randomized design, and the recruitment and data
collection methods have been described in prior publications [@nunez2017;
@carlock2019; @abreu2020]. From 2013 through fall 2016, the study was conducted
at two study sites, one in Pittsburgh, PA, and one in Port St. Lucie, FL. In
January of 2017, the study moved to Athens, GA, and is currently ongoing at
this study site. During the Northern Hemisphere influenza season each year,
individuals who have not yet received an influenza vaccine for the current
season are recruited into the study. At enrollment, individuals complete a
questionnaire providing demographic information, gave a pre-vaccination
blood draw, and are administered a vaccine.

Individuals at least 65 years old were offered a choice between Fluzone
standard dose (SD) and Fluzone high dose (HD; both from Sanofi Pasteur)
vaccines. Individuals under 65 received the SD vaccine. All vaccines were
administered via the intramuscular route. Individuals returned to the study
center to provide a post-vaccination blood sample between 18 and 30 days after
vaccination, with a target of 21 days (2013/14 through 2017/18 flu seasons)
or 28 days (2018/19 flu season onward). Study researchers conducted
hemagglutination inhibition (HAI) assays against the vaccine strains and a
panel of historical strains for each serological sample.

For our study, we used all recorded HAI data collected between the 2013/14
and 2017/18 influenza seasons, as these seasons had a large panel of historical
viruses used for heterologous HAI assays. We only used records which had
both a pre-vaccination and post-vaccination serum sample for the same
individual in a given season.

## Data processing

The outcomes we used for our analyses were all based on the post-vaccination
HAI titer of an individual or the titer increase between
measurements. The HAI titers had a limit of detection of 10 which was coded as
5 in the original dataset. We performed all descriptive analyses on the log
scale using the transformation
$$\log_2 \frac{\text{titer}}{5},$$
which maps $0$ to the LoD. We defined titer increase as the log fold change
of the post-vaccination titer relative to the prevaccination titer:
$$
\text{titer increase} = \log_2 \left( \frac{\text{pre-vaccination titer}}{\text{post-vaccination titer}} \right).
$$
Both of these outcomes are commonly used measurements of the effect of influenza
vaccination in immunogeneicity studies [@falsey2009; @bayer2004].

Our primary explanatory variable used in the study was the antigenic distance
between the strain used in the Fluzone vaccine that a patient received, and
the influenza strain used to conduct an HAI titer with that patient's serum.
Because of this definition, the homologous HAI assay will always have an
antigenic distance of zero, as desired. We computed the pairwise antigenic
distances for all strains used in the study with three methods: a temporal
method based on the year of isolation, a sequence based method called the
dominant *p*-epitope distance, and map distances from antigenic cartography.

The methods for calculating antigenic distances are detailed in the Supplement,
but briefly, we used the absolute difference in the years of isolation for
two strains to compute the temporal distance for those strains. The dominant
*p*-epitope distance measures the proportion of residues which differ across
the five immunodominant epitope sites of the HA head, and the "dominant"
distance is the largest of these proportions [@gupta2006; @pan2010; @pan2016].
Many sequence based distance measures exist, as do many measurements which
incorporate complex biophysical and biochemical information about the HA protein
sequence [@grantham1974] (MORE CITATIONS).
We elected to use the *p*-epitope distance because it is more accurate than
the standard Hamming distance [@gupta2006], but remains trivial to compute
with limited resources. We obtained sequences for all of the influenza strains
used in the cohort study data from either GenBank (CITE) or GISAID (CITE).

The antigenic cartography method is the most complicated. Antigenic cartography
has been previously described [@smith2004], and uses a table of
immunological measurements, such as HAI titers, in a dimension reduction
algorithm to find an acceptable two-dimensional representation of the
immunological data. This two-dimensional representation is called the antigenic
map. We used the HAI panel data for our study to create an antigenic map, and we 
extracted pairwise Euclidean distances between the map coordinates for each of
the influenza strains used in our study.

## Antibody landscape creation

Our primary results are based on calculating metrics from antibody landscapes.
An antibody landscape is a curve generated by plotting how some quantity of
interest varies with antigenic distance. In order to understand
the breadth of an individual's vaccine response, we can calculate summary
metrics from the antibody landscape of their vaccine response, where the
variable of interest is some clinical endpoint like post-vaccination titer
or titer increase.

The individual antibody landscapes are noisy due to sampling variability,
the complex immune histories and demographic characters of different individuals,
and measurement error inherent in HAI titers. So we calculated a "sample
antibody landscape" by fitting a summary curve to all of the individual
antibody landscapes using Bayesian linear regression, which corrected for
censoring in the outcome variables (see Supplement for
details). We fit sample antibody landscapes separately for each season of
data, and we conducted separate analyses for H1N1 and H3N2 strains (since
calculating the antigenic distance between different H1 subtypes would make
within-group distances too small to be useful). Before fitting the sample
antibody landscape, we normalized the antigenic distance within each analysis
stratum, so that we could calculate the summary landscape with each of the
antigenic distance measurements and compare the results.

Our analysis treats each
person-year of data as independent, and we did not model correlations within
individuals, nor did we account for the effects of demographic characteristics
on the sample antibody landscape. For this study, we are primarily interested
in understanding how much information we can extract from the immunological
data and antigenic distance alone.

## Breadth metrics

We extracted three summary metrics from the sample antibody lanscape for each
analysis stratum. These metrics are the intercept of the fitted linear
regression line, which we call the *magnitude* of the response; the proportion
of the summary landscape which was above the clinical protection threshold of
1:40, which we call the *breadth* of the response; and the total area under the
sample antibody landscape (AUC; area under the curve), which we call the
*total strength* of the vaccine response.

In order to validate our novel metrics for response breadth, we calculated
the most common metrics for vaccine breadth that are currently used: the
mean titer increase and post-vaccination titer across all of the strains in
the panel for each individual, and the seroprotection and seroconversion
rates across the panel for each individual. In order to compare these to the
metrics from our sample antibody landscape, we computed the mean of each
metric for the entire sample within each analysis stratum.

We computed all of the metrics for the entire sample in each analysis stratum.
Detailed formulas for calculating each metric are in the Supplement.

## Subsampling analysis

```{r}
#| label: load-sim-parms

# LOAD THE SIM PARMS HERE SO WE CAN DYNAMICALLY SPECIFY
num_strains_each <- 9
num_indiv_each <- 100
num_studies <- 100
```


To test which metrics are the most robust, we cannot fit them to only one
panel. We created several simulated "studies" by subsampling data from
the main cohort study. To create a simulated study, we randomly sampled
$`r num_strains_each`$
historical strains which were used in each season of the study, and also
added the homologous strain for that season to the simulated panel. Again,
we completed this analysis separately for H1N1 and H3N2 influenza data.
With the randomly sampled panel of viruses to use, we randomly sampled
$`r num_indiv_each`$
individuals from the season to create a simulated cohort. This subset of
individuals and historical strains constituted one simulated study. We
repeated the subsampled process multiple times to create
$`r num_studies`$
total simulated studies.

For each study, we estimated the sample antibody landscape using a
Bayesian linear regression model which corrected
for censoring in the outcome, using the same methods as the model on the
entire sample. We then computed the entire set of metrics on each study. In
order to analyze which metrics varied the most over the set of studies, we
computed the coefficient of variation with a $95\%$ bootstrap uncertainty
interval for each metric.

## Case study

To demonstrate the ability of our metrics to compare the vaccine response
between two different candidates, we compared the Fluzone HD and Fluzone SD
vaccines using our cohort data. We first subset the data to only contain
individuals 65 and older, to avoid healthy young details biasing the result
in favor of the SD vaccine. Then, we subset the data into analysis strata by
season, subset, and dose, and we computed the entire set of metrics on each
of the strata. We computed $95\%$ bootstrap uncertainty estimates for each
of the metrics. Finally, we also computed the contrast for each metric
between the two studies, meaning the ratio of the metric for the HD vaccine
to the metric for the SD vaccine, along with a $95\%$ bootstrap uncertainty
interval.

## Implementation

Because our study is a secondary data analysis, any hypothesis tests we
conducted would have low power and inflated error rates as the data were not
collected with our study in mind. Therefore, we elected to conduct all of
our analyses in an exploratory Bayesian framework. While we attempt to
estimate the uncertainty at each stage in the modeling process, we do not
report any $p$-values or significance tests, and our reported uncertainty
intervals should not be interepreted as confirmatory, nor as standard
frequentist confidence intervals.

<!-- TODO update list of packages here -->
We conducted all of our analyses using `r version$version.string` [@base]. We
used the `tidyverse` suite of packages for data cleaning, manipulation, and
visualization [@tidyverse], along with the packages `tidybayes` [@tidybayes] and
`ggdist` [@ggdist1; @ggdist2]. We used the packages `here` [@here], `renv`
[@renv], and `box` [@box] for code management. We used `ggplot2` for generating
all figures [@ggplot2]. We used the packages `gtsummary` [@gtsummary] and
`flextable` [@flextable] for making all tables.

We compiled our manuscript using Quarto version **UPDATE VERSION**
[@Allaire_Quarto_2024] with the R packages
`knitr` [@knitr1; @knitr2; @knitr3], and `softbib` [@softbib].
We implemented our bayesian models using `cmdstanr` [@cmdstanr] with cmdstan
version **VERSION NUMBER** [@gabry2023] as the interface to the Stan
probabilistic programming language [@standevelopmentteam2022; @carpenter2017a].
More details on implementation are included in the Supplement, along with
instructions for reproducing our results. The code and data are archived on
Zenodo at this link:
**LINK HERE**.

# Results

OK sounds good. Right now I am thinking table 1 -> a figure showing the antibody landscapes (or scatterplot) and how the strains move with the different metrics -> table and/or figuring showing the subsampling results -> case study with HD/SD (1 fig / 1 tab or 2 figs).

* Table 1 and cohort demographics
* Figure showing scatterplots, like combined version of figure 1, 2, 3, that
only shows titer increase and shows the normalized distance with a second axis.
* Table or figure showing the subsampling results
* Case study with HD/SD

## Data description

```{r}
#| label: load tot_n
#| eval: false

# Total number of study subjects here
tot_n <- readr::read_rds(here::here("results", "files", "n-indiv.Rds"))
```

text here

```{r tbl-demographics}
#| label: tbl-demographics
#| eval: false
#| tbl-cap: |
#|   Tab.

# PUT TABLE ONE HERE
readr::read_rds(here::here("results", "tables", "demographics-person-years-bot.Rds")) |>
	flextable::autofit()
```

## Result header

text here


# Discussion

text here

# Followup paper ideas

* Comparing cartographies from different datasets and generated in different ways via procrustes analyses -- then compare the differences in pairwise antigenic distances to see if it practically matters
* Methodological paper where we explore the practical implications of the censoring correction and when it matters or doesn't (working in methods for censored outcome and predictor is probably important here too, need to analyze how important both are) (or potentially comparing those methods with doing a full longitudinal analysis of pre/post titers that can control for censoring all in the outcome).
* Use our study and potentially multiple others, do a subsampling analysis, and try to use the matrix completion or other imputation methods. Basically do a sensitivity analysis and see how much imputation would have changed our conclusions.
* Analyze change in breadth over time, like Ben Cowling's idea. Basically we would calculate the summary landscape for each season and look at the trend in metrics.

# Acknowledgments

We would like to acknowledge Michael Carlock (Cleveland Clinic, Port St. Lucie,
FL, USA) for assistance with obtaining data.

**Conflicts of interest**: None of the authors declare any conflicts of interest.

**Funding sources**: 
AH received partial support from NIH grants/contracts U01AI150747, R01AI170116,
75N93019C00052 and 75N93021C00018.
The funders had no role in the study design, data collection and analysis,
decision to publish, or preparation of the manuscript.

**Author contributions**:



**Data availability statement**: an archive containing all of the code files
and data necessary to reproduce our results is located on Zenodo at this link:
**LINK HERE**.

# References

::: {#refs}
:::

<!-- END OF FILE -->
